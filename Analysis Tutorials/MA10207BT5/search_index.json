[["index.html", "Analysis 1B — Tutorial 5 Introduction", " Analysis 1B — Tutorial 5 Christian Jones: University of Bath March 2023 Introduction Here is the material to accompany the 5th Analysis 1B Tutorial on the 6th March. Alternative formats can be downloaded by clicking the download icon at the top of the page. Please send any comments or corrections to Christian Jones (caj50). To return to the homepage, click here. "],["lecture-recap.html", "1 Lecture Recap", " 1 Lecture Recap After what was mainly revision last week, we’re moving onto some new stuff again! It turns out there’s still a bit we can say about continuity, especially on compact intervals. Finally, we’re going to look at differentiation, which gives us a way of describing how fast a function changes. "],["inverse-functions.html", "1.1 Inverse Functions", " 1.1 Inverse Functions A particularly useful class of functions we may be interested in are known as invertible. These functions \\(f: A \\to B\\) provide a way of moving between sets \\(A\\) and \\(B\\) (and back again) without losing any information about \\(A\\) and \\(B\\). Before we talk about them in more detail, it’s worth recalling some defintions: Definition 1.1: (Injectivity, Surjectivity and Bijectivity) Let \\(f: A \\to B\\) be a function. If \\(\\forall x, y \\in B\\) with \\(x \\neq y\\), \\(f(x) = f(y) \\implies x = y\\), then \\(f\\) is said to be injective. If \\(\\forall y \\in B\\), \\(\\exists x \\in A\\) such that \\(f(x) = y\\), then \\(f\\) is surjective. If \\(f\\) is both injective and surjective, then it is called bijective. In words, bijectivity means that for a function \\(f:A \\to B\\), every element in the codomain \\(B\\) is mapped to by a unique element in the domain \\(A\\). These bijective functions are said to be invertible, that is, there exists an inverse function \\(f^{-1}: B \\to A\\) such that \\(f^{-1} \\circ f\\) and \\(f \\circ f^{-1}\\) produce the identity maps on \\(A\\) and \\(B\\) respectively. Now that we have these definitions, we can say something useful about the continuity of inverse functions: Theorem 1.1: Let \\(I \\subseteq \\mathbb{R}\\) be a non-empty1 interval, and let \\(f: I \\to \\mathbb{R}\\) be continuous on \\(I\\). Assume that \\(f\\) is strictly increasing2 (or strictly decreasing) on \\(I\\). Then: \\(f(I)\\) is an interval, \\(f : I \\to f(I) =: J\\) is bijective, and \\(f^{-1}: J \\to I\\) is continuous on \\(J\\). For a specific example, consider the exponential function \\(\\exp:\\mathbb{R} \\to \\mathbb{R}\\) defined by \\[f(x) = \\sum_{n = 0}^{\\infty} \\frac{x^n}{n!} = \\mathrm{e}^x.\\] Since \\(\\mathbb{R}\\) is a non-empty interval, and in semester 1, we showed that \\(\\exp\\) was continuous and strictly increasing, this theorem applies. Semester 1 material also gave us that \\(\\exp(\\mathbb{R}) = (0, \\inf)\\), so \\(\\exp: \\mathbb{R} \\to (0, \\infty)\\) is a bijection. Hence, the final point in this theorem gives us that the inverse function \\(\\log: (0,\\infty) \\to \\mathbb{R}\\) is continuous on \\((0,\\infty)\\). This example actually turns out to be really useful if we’re dealing with sequences, as we can now prove the following — hugely general — result: Proposition 1.2: Let \\((a_n)_n\\) and \\((b_n)\\) be real sequences such that \\(\\lim_{n\\to\\infty}a_n = a\\) and \\(\\lim_{n\\to\\infty}b_n = b\\). If \\(a&gt;0\\), then \\[\\lim_{n \\to \\infty} a_n^{b_n} = \\left(\\lim_{n\\to\\infty} a_n\\right)^{\\lim_{n\\to\\infty}b_n}.\\] Proof. Since \\(\\lim_{n \\to \\infty}a_n = a &gt; 0\\), then \\(\\exists N\\in\\mathbb{N}\\) such that \\(\\forall n \\geq N\\), \\[\\lvert a_n - a \\rvert &lt; \\frac{a}{2} \\Longleftrightarrow \\frac{a}{2} &lt; a_n &lt; \\frac{3a}{2}.\\] In particular, \\(a_n &gt; 0\\) for all \\(n \\geq N\\). Now, for \\(n \\geq N\\), \\[\\begin{align*} a_n^{b_n} &amp;= \\exp\\left(\\log\\left(a_n^{b_n}\\right)\\right)\\;\\;\\text{(as $a_n &gt; 0$)},\\\\ &amp;= \\exp\\left(b_n\\log\\left(a_n\\right)\\right)\\;\\;\\text{(properties of $\\log$)}. \\end{align*}\\] So as \\(n \\to \\infty\\) since both \\(\\exp\\) and \\(\\log\\) are continuous, \\[\\begin{align*} a_n^{b_n} &amp;\\to \\exp\\left(b\\log(a)\\right)\\\\ &amp;= \\exp\\left(\\log\\left(a^b\\right)\\right),\\\\ &amp;= a^b. \\end{align*}\\] □ This is so we can talk about surjectivity.↩︎ In other words, for \\(x &lt; y\\) in \\(I\\), \\(f(x) &lt; f(y)\\).↩︎ "],["weierstrass-extremal-theorem.html", "1.2 Weierstrass Extremal Theorem", " 1.2 Weierstrass Extremal Theorem Much like the Intermediate Value Theorem, we can obtain some special continuity results when our functions are defined on compact (i.e. closed and bounded) intervals. This result is stated below: Theorem 1.3: (Weierstrass Extremal Theorem (WET)) Let \\(a,b \\in \\mathbb{R}\\) with \\(a&lt;b\\), and let \\(f \\in C^{0}([a,b])\\). Then: \\(f\\) is bounded: \\[\\exists M &gt; 0 \\;\\;\\text{s.t.}\\;\\; \\lvert f(x) \\rvert \\leq M\\;\\;\\forall x \\in [a,b].\\] \\(f\\) attains its bounds: \\[\\exists p,q \\in [a,b] \\;\\; \\text{s.t}\\;\\; \\forall x \\in [a,b], f(q) \\leq f(x) \\leq f(p).\\] This last point states that if \\(f \\in C^{0}([a,b])\\), \\[\\sup_{x \\in [a,b]}f(x) = \\max_{x\\in[a,b]}f(x)\\;\\;\\text{and}\\;\\;\\inf_{x \\in [a,b]}f(x) = \\min_{x\\in[a,b]}f(x).\\] So in fact, what this theorem tells us is that for a function defined on a compact interval, we have some control on its growth, and we know that the function has a maximum and minimum value! This can be seen pictorally in Figure ?? "],["differentiation.html", "1.3 Differentiation", " 1.3 Differentiation While functions are very good at describing physical quantities such as temperature, density or momentum we can usually gain more insight into these variables by studying how fast they change at a given position or time. For example, Newton’s 2nd Law states that the resultant force on an object is equal to the rate of change of momentum with respect to time. Mathematically, we study rates of change using derivatives, which again relies on the ideas behind limits! 1.3.1 The Definition Definition 1.2: (Derivative) Let \\(f: D \\to \\mathbb{R}\\), where \\(D \\subseteq \\mathbb{R}\\) is an open set, and let \\(c \\in D\\). Then, if \\(\\exists L \\in \\mathbb{R}\\) such that \\[\\lim_{h \\to 0}\\frac{f(c+h) - f(c)}{h} = L,\\] we say that \\(f\\) is differentiable at \\(c\\), and call \\(L\\) the derivative of \\(f\\) at \\(c\\). We can note a few things here: Firstly, if this \\(L\\) exists, we write it as \\(f&#39;(c)\\) to make it clear that its a derivative. We require \\(D\\) to be open, so that we can actually take limits! If, for example, \\(D = [-1,2]\\), we could attempt to define the derivative at any point in the interior of \\(D\\), \\(D^{\\circ} = (-1,2)\\), but we couldn’t define the derivative at \\(x = -1\\) or \\(x = 2\\).3 Substituting \\(x = c+h\\) into the definition gives us that equivalently \\(f\\) is differentiable at \\(c\\) if there exists \\(L\\in\\mathbb{R}\\) such that \\[\\lim_{x \\to c}\\frac{f(x) - f(c)}{x - c} = L.\\] One quick result we obtain from this definition is the following: Proposition 1.4: If a function \\(f:D \\to \\mathbb{R}\\) is differentiable at a point \\(c\\), then it is continuous at \\(c\\). The contrapositive of this is very useful for ruling functions out: if a function is not continuous, it is not differentiable. As a final remark, or warning, continuity does not imply differentiability! To see this, think of either \\(f(x) = \\lvert x \\rvert\\) at \\(x = 0\\), or look up the Weierstrass function. There is nothing stopping us; however, defining left and right derivatives at these points, e.g. we could search for \\[\\lim_{x \\to -1^{+}}\\frac{f(-1+h) - f(-1)}{h}\\;\\;\\text{or}\\;\\;\\lim_{x \\to 2^{-}}\\frac{f(2+h) - f(2)}{h}.\\]↩︎ "],["hints.html", "2 Hints", " 2 Hints As per usual, here’s where you’ll find the problem sheet hints! This one is largely similar to the one that was covered in tutorials — you just need to be a bit more careful when verifying the hypothesis of the theorem involving inverse functions. When proving bijectivity, you can use results from tutorial question 1 to help too! There are couple of ways to do this, but in each, you need to calculate the limit of the difference quotient, i.e. \\[\\lim_{x\\to 0 }\\frac{f(x) - f(0)}{x - 0}.\\] Try using sequences! (Or if you’ve seen it, try the function version of the pinching theorem). Have you learnt any good theorems involving maxima/minima of functions lately? "]]
